{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "DecisionTreeClassifier(max_depth=2)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:, 2:]\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(\n",
    "            tree_clf,\n",
    "            out_file=\"./iris_tree.dot\",\n",
    "            feature_names=iris.feature_names[2:],\n",
    "            class_names=iris.target_names,\n",
    "            rounded=True,\n",
    "            filled=True\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "! dot -Tpng iris_tree.dot -o iris_tree.png"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "One of the many qualities of Decision Trees is that they require very little data preparation. In particular, they don’t require feature scaling or centering at all.\n",
    "\n",
    "* Gini impurity:\n",
    "\n",
    " $$G_{i}=1-\\sum_{k=1}^{n} p_{i, k}^{2}$$\n",
    "\n",
    " $p_{i,k}$ is the ratio of class $k$ instances among the training instances in the $i$th node.\n",
    "\n",
    "Scikit-Learn uses the CART algorithm, which produces only binary trees: nonleaf nodes always have two children\n",
    "(i.e., questions only have yes/no answers). However, other algorithms such as ID3 can produce Decision Trees with\n",
    "nodes that have more than two children."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.        , 0.90740741, 0.09259259]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5, 1.5]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Scikit-Learn uses the Classification And Regression Tree (CART) algorithm to train Decision Trees (also called “growing” trees). The idea is really quite simple: the algo‐ rithm first splits the training set in two subsets using a single feature\n",
    "$k$ and a threshold $t_k$ (e.g., “petal length ≤ 2.45 cm”). How does\n",
    "it choose $k$ and $t_k$? It searches for the pair $(k, t_k)$ that\n",
    "produces the purest subsets (weighted by their size). The cost\n",
    "function that the algorithm tries to minimize is given by\n",
    "\\begin{align}\n",
    "\\begin{array}{l}\n",
    "J\\left(k, t_{k}\\right)=\\frac{m_{\\text {left }}}{m} G_{\\text {left }}+\\frac{m_{\\text {right }}}{m} G_{\\text {right }} \\\\\n",
    "\\text { where }\\left\\{\\begin{array}{l}\n",
    "G_{\\text {left/right }} \\text { measures the impurity of the left/right subset, } \\\\\n",
    "m_{\\text {left/right }} \\text { is the number of instances in the left/right subset. }\n",
    "\\end{array}\\right.\n",
    "\\end{array}\n",
    "\\end{align}\n",
    "\n",
    "Once it has successfully split the training set in two, it splits the\n",
    "subsets using the same logic, then the sub-subsets and so on, recursively.\n",
    "It stops recursing once it reaches the maximum\n",
    "depth (defined by the max_depth hyperparameter), or if it cannot find\n",
    "a split that will reduce impurity. A few other hyperparameters\n",
    "(described in a moment) control additional stopping conditions\n",
    "(min_samples_split, min_sam ples_leaf, min_weight_fraction_leaf,\n",
    "and max_leaf_nodes).\n",
    "\n",
    "finding the optimal tree is known to be an NP-Complete problem:2 it requires $O(\\exp(m))$ time,\n",
    " making the problem intractable even for fairly small training sets. This is why we must settle for a “reasonably good” solution.\n",
    "\n",
    "#### Gini Impurity or Entropy?\n",
    "\n",
    "$$H_{i}=-\\sum_{k=1 \\atop p_{i, k} \\neq 0}^{n} p_{i, k} \\log _{2}\\left(p_{i, k}\\right)$$\n",
    "\n",
    "So should you use Gini impurity or entropy? The truth is, most of the time it does not make a big difference: they lead to similar trees. Gini impurity is slightly faster to compute, so it is a good default. However, when they differ, Gini impurity tends to isolate the most frequent class in its own branch of the tree, while entropy tends to produce slightly more balanced trees.\n",
    "\n",
    "\n",
    "### Regularization Hyperparameters\n",
    "\n",
    "Such a model is often called a nonparametric model, not because it does not have any parameters (it often has a lot) but because the number of parameters is not determined prior to training, so the model structure is free to stick closely to the data.\n",
    "\n",
    "In contrast, a parametric model such as a linear model has a predetermined number of parameters, so its degree of freedom is limited, reducing the risk of overfitting (but increasing the risk of underfitting).\n",
    "\n",
    "The DecisionTreeClassifier class has a few other parameters that similarly restrict the shape of the Decision Tree:\n",
    "1. min_samples_split (the minimum number of samples a node must have before it can be split)\n",
    "2. min_samples_leaf (the minimum number of samples a leaf node must have)\n",
    "3. min_weight_fraction_leaf (same as min_samples_leaf but expressed as a fraction of the total number of weighted\n",
    "instances)\n",
    "4. max_leaf_nodes (maximum number of leaf nodes)\n",
    "5. max_features (maximum number of features that are evaluated for splitting at each node). Increasing min_* hyperparameters or reducing max_* hyperparameters will regularize the model.\n",
    "\n",
    "Other algorithms work by first training the Decision Tree without restrictions, then pruning (deleting) unnecessary nodes. A node whose children are all leaf nodes is considered unnecessary if the purity improvement it provides is not statistically significant. Stan‐ dard statistical tests, such as the χ2 test, are used to estimate the probability that the improvement is purely the result of chance (which is called the null hypothesis). If this probability, called the p- value, is higher than a given threshold (typically 5%, controlled by a hyperparameter), then the node is considered unnecessary and its children are deleted. The pruning continues until all unnecessary nodes have been pruned.\n",
    "\n",
    "### Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "DecisionTreeRegressor(max_depth=2)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg.fit(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       1.09259259, 1.09259259, 1.09259259, 1.09259259, 1.09259259,\n       1.09259259, 1.09259259, 1.09259259, 1.09259259, 1.09259259,\n       1.09259259, 1.09259259, 1.09259259, 1.09259259, 1.09259259,\n       1.09259259, 1.09259259, 1.09259259, 1.09259259, 1.09259259,\n       1.97826087, 1.09259259, 1.09259259, 1.09259259, 1.09259259,\n       1.09259259, 1.09259259, 1.09259259, 1.09259259, 1.09259259,\n       1.09259259, 1.09259259, 1.09259259, 1.09259259, 1.09259259,\n       1.09259259, 1.09259259, 1.09259259, 1.09259259, 1.09259259,\n       1.09259259, 1.09259259, 1.09259259, 1.09259259, 1.09259259,\n       1.09259259, 1.09259259, 1.09259259, 1.09259259, 1.09259259,\n       1.97826087, 1.97826087, 1.97826087, 1.97826087, 1.97826087,\n       1.97826087, 1.09259259, 1.97826087, 1.97826087, 1.97826087,\n       1.97826087, 1.97826087, 1.97826087, 1.97826087, 1.97826087,\n       1.97826087, 1.97826087, 1.97826087, 1.97826087, 1.09259259,\n       1.97826087, 1.97826087, 1.97826087, 1.97826087, 1.97826087,\n       1.97826087, 1.97826087, 1.97826087, 1.97826087, 1.09259259,\n       1.97826087, 1.97826087, 1.97826087, 1.09259259, 1.09259259,\n       1.97826087, 1.97826087, 1.97826087, 1.97826087, 1.97826087,\n       1.97826087, 1.97826087, 1.97826087, 1.97826087, 1.97826087,\n       1.97826087, 1.97826087, 1.97826087, 1.97826087, 1.97826087])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_reg.predict(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The CART algorithm works mostly the same way as earlier, except that instead of try‐ ing to split the training set in a way that minimizes impurity, it now tries to split the training set in a way that minimizes the MSE. Equation 6-4 shows the cost function that the algorithm tries to minimize.\n",
    "\n",
    "\\begin{align*}\n",
    "&J\\left(k, t_{k}\\right)=\\frac{m_{\\mathrm{left}}}{m} \\mathrm{MSE}_{\\mathrm{left}}+\\frac{m_{\\text {right }}}{m} \\mathrm{MSE}_{\\text {right }}\\\\\n",
    "&\\text { where }\\left\\{\\begin{array}{l}\n",
    "\\mathrm{MSE}_{\\text {node }}=\\sum_{i \\in \\text { node }}\\left(\\hat{y}_{\\text {node }}-y^{(i)}\\right)^{2} \\\\\n",
    "\\hat{y}_{\\text {node }}=\\frac{1}{m_{\\text {node }}} \\sum_{i \\in \\operatorname{node}} y^{(i)}\n",
    "\\end{array}\\right.\n",
    "\\end{align*}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Instability\n",
    "\n",
    "Decision Trees have a lot going for them: they are simple to understand and interpret, easy to use, versatile, and powerful.\n",
    "There are few limitations:\n",
    "* prefer orthogonal decision boundaries, which sensitive to training set rotation.\n",
    "* More generally, the main issue with Decision Trees is that they are very sensitive to small variations in the training data.\n",
    "\n",
    "\n",
    "Actually, since the training algorithm used by Scikit-Learn is stochastic6 you may get very different models even on the same training data (unless you set the random_state hyperparameter)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}