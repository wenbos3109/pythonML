{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3*X + np.random.rand(100, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'bo',  markersize=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "print(X_b.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "theta_best"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = X_b.dot(theta_best)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'bo',  markersize=3)\n",
    "plt.plot(X, y_pred, 'r-', linewidth=0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The LinearRegression class is based on the scipy.linalg.lstsq() function (the name stands for “least squares”), which you could call directly:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
    "theta_best_svd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This function computes $\\widehat{\\boldsymbol{\\theta}}=\\mathbf{X}^{+} \\mathbf{y}$, where $X^+$\n",
    "is the pseudoinverse of $X$, You can use np.linalg.pinv() to compute the pseudoin‐ verse directly:\n",
    "\n",
    "### what is svd?\n",
    "\n",
    "To compute the matrix Σ+, the algorithm takes Σ and sets to zero all values smaller than a tiny threshold value, then it replaces all the non-zero values with their inverse, and finally it transposes the resulting matrix. This approach is more efficient than computing the Normal Equation, plus it handles edge cases nicely: indeed, the Normal Equation may not work if the matrix XTX is not invertible (i.e., singular), such as if m < n or if some features are redundant, but the pseudoinverse is always defined."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.linalg.pinv(X_b).dot(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gradient descent\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial}{\\partial \\theta_{j}} \\operatorname{MSE}(\\boldsymbol{\\theta}) & =\\frac{2}{m} \\sum_{i=1}^{m}\\left(\\boldsymbol{\\theta}^{T} \\mathbf{x}^{(i)}-y^{(i)}\\right) x_{j}^{(i)}\\\\\n",
    "    \\nabla_{\\boldsymbol{\\theta}} \\operatorname{MSE}(\\boldsymbol{\\theta}) &=\\left(\\begin{array}{c}\n",
    "\\frac{\\partial}{\\partial \\theta_{0}} \\operatorname{MSE}(\\boldsymbol{\\theta}) \\\\\n",
    "\\frac{\\partial}{\\partial \\theta_{1}} \\operatorname{MSE}(\\boldsymbol{\\theta}) \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial}{\\partial \\theta_{n}} \\operatorname{MSE}(\\boldsymbol{\\theta})\n",
    "\\end{array}\\right)=\\frac{2}{m} \\mathbf{X}^{T}(\\mathbf{X} \\boldsymbol{\\theta}-\\mathbf{y})\n",
    "\\end{align*}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "eta = 0.001 # learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "plt.plot(X, y, 'bo',  markersize=3)\n",
    "loss = []\n",
    "theta = np.random.randn(2, 1)\n",
    "for i in range(n_iterations):\n",
    "    y_pred = X_b.dot(theta)\n",
    "    if i % 100 == 0:\n",
    "        plt.plot(X, y_pred, 'r-', linewidth=0.5)\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta)-y)\n",
    "    theta = theta - eta * gradients\n",
    "    loss.append(mean_squared_error(y, y_pred))\n",
    "print(theta)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(np.arange(n_iterations), loss, 'b-')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stochastic gradient descent\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "t0, t1 = 5, 50 # learning schedule hyperparameters\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)\n",
    "theta = np.random.randn(2,1) # random initialization\n",
    "loss = []\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients\n",
    "        y_pred = X_b.dot(theta)\n",
    "        loss.append(mean_squared_error(y, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(loss)), loss, 'b-')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# penalty=None: no regularization\n",
    "# tol=1e-3: stop when loss is less than 1e-3\n",
    "# eta0: learning rate\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\n",
    "sgd_reg.fit(X, y.ravel())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sgd_reg.intercept_, sgd_reg.coef_\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Polynomial regression\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
    "plt.plot(X, y, 'bo', markersize=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "X[0], X[0]**2, X_poly[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "lin_reg.coef_, lin_reg.intercept_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_plot = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "X_plot_poly = poly_features.fit_transform(X_plot)\n",
    "print(X_plot_poly.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'bo', markersize=3)\n",
    "plt.plot(np.linspace(-3, 3, 100),lin_reg.predict(X_plot_poly), 'r-')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Learning curve"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "def plot_learning_curves(model, X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "    train_errors, val_errors = [], []\n",
    "    for m in range(1, len(X_train)):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "plot_learning_curves(lin_reg, X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "polynomial_regression = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n",
    "        (\"lin_reg\", LinearRegression()),\n",
    "    ])\n",
    "plot_learning_curves(polynomial_regression, X, y)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Bias/Variance Tradeoff\n",
    "An important theoretical result of statistics and Machine Learning is the fact that a model’s generalization error can be expressed as the sum of three very different errors:\n",
    "1. Bias: This part of the generalization error is due to wrong assumptions, such as assum‐ ing that the data is linear when it is actually quadratic. A high-bias model is most likely to underfit the training data.\n",
    "2. Variance: This part is due to the model’s excessive sensitivity to small variations in the training data. A model with many degrees of freedom (such as a high-degree pol‐ ynomial model) is likely to have high variance, and thus to overfit the training data.\n",
    "3. Irreducible error: This part is due to the noisiness of the data itself. The only way to reduce this part of the error is to clean up the data (e.g., fix the data sources, such as broken sensors, or detect and remove outliers).\n",
    "Increasing a model’s complexity will typically increase its variance and reduce its bias. Conversely, reducing a model’s complexity increases its bias and reduces its variance. This is why it is called a tradeoff.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# two ways\n",
    "poly_features = PolynomialFeatures(degree=40, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "ridge_reg = Ridge(alpha=0, solver='cholesky')\n",
    "ridge_reg.fit(X_poly, y)\n",
    "print(ridge_reg.coef_, ridge_reg.intercept_)\n",
    "\n",
    "# or add penalty\n",
    "# sgd_reg = SGDRegressor(penalty='l2')\n",
    "# sgd_reg.fit(X, y.ravel())\n",
    "\n",
    "X_plot = poly_features.fit_transform(np.linspace(-3, 3, 100).reshape(-1, 1))\n",
    "plt.plot(X, y, 'bo', markersize=3)\n",
    "plt.plot(np.linspace(-3, 3, 100), ridge_reg.predict(X_plot), 'r-')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Lasso regression\n",
    "\n",
    "#### Why Lasso results in sparsity\n",
    "\n",
    "Consider a vector $\\vec{x}=(1, \\varepsilon) \\in \\mathbb{R}^{2}$, where $\\epsilon>0$\n",
    "is small, then $l_1$ and $l_2$ norms of $\\vec{x}$ are given by\n",
    "$$\\|\\vec{x}\\|_{1}=1+\\varepsilon,\\|\\vec{x}\\|_{2}^{2}=1+\\varepsilon^{2}$$\n",
    "\n",
    "After introducing some regularization procedure, we are going to reduce the\n",
    "magnitude of one of the elements of $\\vec{x}$ by $\\delta<\\epsilon$, if we change\n",
    "$x_1$ to $1-\\delta$, then\n",
    "$$\\|\\vec{x}-(\\delta, 0)\\|_{1}=1-\\delta+\\varepsilon,\\|\\vec{x}-(\\delta, 0)\\|_{2}^{2}=1-2 \\delta+\\delta^{2}+\\varepsilon^{2}$$\n",
    "\n",
    "then reduction of norm are\n",
    "\n",
    "\\begin{align}\n",
    "\\Delta l_1 &=\\delta\\\\\n",
    "\\Delta l_2 &=2\\delta-\\delta^2\n",
    "\\end{align}\n",
    "\n",
    "On the other hand, if we reduce $x_2$, the smaller one by $\\delta$,\n",
    "$$\\|\\vec{x}-(0, \\delta)\\|_{1}=1-\\delta+\\varepsilon, \\quad\\|\\vec{x}-(0, \\delta)\\|_{2}^{2}=1-2 \\varepsilon \\delta+\\delta^{2}+\\varepsilon^{2}$$\n",
    "\n",
    "the reduction would be\n",
    "\n",
    "\\begin{align}\n",
    "    \\Delta l_1 &=\\delta\\\\\n",
    "    \\Delta l_2 &=2\\epsilon\\delta-\\delta^2\n",
    "\\end{align}\n",
    "\n",
    "Thus, when penalizing a moedel using $l_2$ norm, it is highly likely that\n",
    "anything will ever be set to zero, because $\\epsilon$ is small and $\\delta$ is even\n",
    "smaller."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "poly_features = PolynomialFeatures(degree=10)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "lasso_reg = Lasso(alpha=1)\n",
    "lasso_reg.fit(X_poly, y)\n",
    "print(lasso_reg.coef_, lasso_reg.intercept_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Logistic regression\n",
    "\n",
    "The cost function is\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\boldsymbol{\\theta})=-\\frac{1}{m} \\sum_{i=1}^{m}\\left[y^{(i)} \\log \\left(\\hat{p}^{(i)}\\right)+\\left(1-y^{(i)}\\right) \\log \\left(1-\\hat{p}^{(i)}\\right)\\right]\n",
    "\\end{equation}\n",
    "\n",
    "Derivatives is\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_{j}} \\mathrm{~J}(\\boldsymbol{\\theta})=\\frac{1}{m} \\sum_{i=1}^{m}\\left(\\sigma\\left(\\boldsymbol{\\theta}^{T} \\mathbf{x}^{(i)}\\right)-y^{(i)}\\right) x_{j}^{(i)}$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "list(iris.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "iris['feature_names']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "iris['data'][:, 3:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = iris['data'][:, 3:]\n",
    "y = (iris['target'] == 2).astype(np.int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", label=\"Iris-Virginica\")\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", label=\"Not Iris-Virginica\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(y_proba)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Softmax function\n",
    "\n",
    "$$\\hat{p}_{k}=\\sigma(\\mathbf{s}(\\mathbf{x}))_{k}=\\frac{\\exp \\left(s_{k}(\\mathbf{x})\\right)}{\\sum_{j=1}^{K} \\exp \\left(s_{j}(\\mathbf{x})\\right)}$$\n",
    "\n",
    "the prediction is\n",
    "\n",
    "$$\\hat{y}=\\underset{k}{\\operatorname{argmax}} \\sigma(\\mathbf{s}(\\mathbf{x}))_{k}=\\underset{k}{\\operatorname{argmax}} s_{k}(\\mathbf{x})=\\underset{k}{\\operatorname{argmax}}\\left(\\left(\\boldsymbol{\\theta}^{(k)}\\right)^{T} \\mathbf{x}\\right)$$\n",
    "\n",
    "the cross entropy loss is\n",
    "\n",
    "$$J(\\boldsymbol{\\Theta})=-\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y_{k}^{(i)} \\log \\left(\\hat{p}_{k}^{(i)}\\right)$$\n",
    "\n",
    "Gradient vector for class $k$\n",
    "\n",
    "$$\\nabla_{\\boldsymbol{\\theta}^{(k)}} J(\\boldsymbol{\\Theta})=\\frac{1}{m} \\sum_{i=1}^{m}\\left(\\hat{p}_{k}^{(i)}-y_{k}^{(i)}\\right) \\mathbf{x}^{(i)}$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "th#%%\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3*X + np.random.rand(100, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'bo',  markersize=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "print(X_b.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "theta_best"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = X_b.dot(theta_best)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'bo',  markersize=3)\n",
    "plt.plot(X, y_pred, 'r-', linewidth=0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The LinearRegression class is based on the scipy.linalg.lstsq() function (the name stands for “least squares”), which you could call directly:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
    "theta_best_svd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This function computes $\\widehat{\\boldsymbol{\\theta}}=\\mathbf{X}^{+} \\mathbf{y}$, where $X^+$\n",
    "is the pseudoinverse of $X$, You can use np.linalg.pinv() to compute the pseudoin‐ verse directly:\n",
    "\n",
    "### what is svd?\n",
    "\n",
    "To compute the matrix Σ+, the algorithm takes Σ and sets to zero all values smaller than a tiny threshold value, then it replaces all the non-zero values with their inverse, and finally it transposes the resulting matrix. This approach is more efficient than computing the Normal Equation, plus it handles edge cases nicely: indeed, the Normal Equation may not work if the matrix XTX is not invertible (i.e., singular), such as if m < n or if some features are redundant, but the pseudoinverse is always defined."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.linalg.pinv(X_b).dot(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gradient descent\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial}{\\partial \\theta_{j}} \\operatorname{MSE}(\\boldsymbol{\\theta}) & =\\frac{2}{m} \\sum_{i=1}^{m}\\left(\\boldsymbol{\\theta}^{T} \\mathbf{x}^{(i)}-y^{(i)}\\right) x_{j}^{(i)}\\\\\n",
    "    \\nabla_{\\boldsymbol{\\theta}} \\operatorname{MSE}(\\boldsymbol{\\theta}) &=\\left(\\begin{array}{c}\n",
    "\\frac{\\partial}{\\partial \\theta_{0}} \\operatorname{MSE}(\\boldsymbol{\\theta}) \\\\\n",
    "\\frac{\\partial}{\\partial \\theta_{1}} \\operatorname{MSE}(\\boldsymbol{\\theta}) \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial}{\\partial \\theta_{n}} \\operatorname{MSE}(\\boldsymbol{\\theta})\n",
    "\\end{array}\\right)=\\frac{2}{m} \\mathbf{X}^{T}(\\mathbf{X} \\boldsymbol{\\theta}-\\mathbf{y})\n",
    "\\end{align*}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "eta = 0.001 # learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "plt.plot(X, y, 'bo',  markersize=3)\n",
    "loss = []\n",
    "theta = np.random.randn(2, 1)\n",
    "for i in range(n_iterations):\n",
    "    y_pred = X_b.dot(theta)\n",
    "    if i % 100 == 0:\n",
    "        plt.plot(X, y_pred, 'r-', linewidth=0.5)\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta)-y)\n",
    "    theta = theta - eta * gradients\n",
    "    loss.append(mean_squared_error(y, y_pred))\n",
    "print(theta)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(np.arange(n_iterations), loss, 'b-')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stochastic gradient descent\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "t0, t1 = 5, 50 # learning schedule hyperparameters\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)\n",
    "theta = np.random.randn(2,1) # random initialization\n",
    "loss = []\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients\n",
    "        y_pred = X_b.dot(theta)\n",
    "        loss.append(mean_squared_error(y, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(loss)), loss, 'b-')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# penalty=None: no regularization\n",
    "# tol=1e-3: stop when loss is less than 1e-3\n",
    "# eta0: learning rate\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\n",
    "sgd_reg.fit(X, y.ravel())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sgd_reg.intercept_, sgd_reg.coef_\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Polynomial regression\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
    "plt.plot(X, y, 'bo', markersize=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "X[0], X[0]**2, X_poly[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "lin_reg.coef_, lin_reg.intercept_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_plot = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "X_plot_poly = poly_features.fit_transform(X_plot)\n",
    "print(X_plot_poly.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'bo', markersize=3)\n",
    "plt.plot(np.linspace(-3, 3, 100),lin_reg.predict(X_plot_poly), 'r-')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Learning curve"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "def plot_learning_curves(model, X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "    train_errors, val_errors = [], []\n",
    "    for m in range(1, len(X_train)):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "plot_learning_curves(lin_reg, X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "polynomial_regression = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n",
    "        (\"lin_reg\", LinearRegression()),\n",
    "    ])\n",
    "plot_learning_curves(polynomial_regression, X, y)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Bias/Variance Tradeoff\n",
    "An important theoretical result of statistics and Machine Learning is the fact that a model’s generalization error can be expressed as the sum of three very different errors:\n",
    "1. Bias: This part of the generalization error is due to wrong assumptions, such as assum‐ ing that the data is linear when it is actually quadratic. A high-bias model is most likely to underfit the training data.\n",
    "2. Variance: This part is due to the model’s excessive sensitivity to small variations in the training data. A model with many degrees of freedom (such as a high-degree pol‐ ynomial model) is likely to have high variance, and thus to overfit the training data.\n",
    "3. Irreducible error: This part is due to the noisiness of the data itself. The only way to reduce this part of the error is to clean up the data (e.g., fix the data sources, such as broken sensors, or detect and remove outliers).\n",
    "Increasing a model’s complexity will typically increase its variance and reduce its bias. Conversely, reducing a model’s complexity increases its bias and reduces its variance. This is why it is called a tradeoff.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# two ways\n",
    "poly_features = PolynomialFeatures(degree=40, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "ridge_reg = Ridge(alpha=0, solver='cholesky')\n",
    "ridge_reg.fit(X_poly, y)\n",
    "print(ridge_reg.coef_, ridge_reg.intercept_)\n",
    "\n",
    "# or add penalty\n",
    "# sgd_reg = SGDRegressor(penalty='l2')\n",
    "# sgd_reg.fit(X, y.ravel())\n",
    "\n",
    "X_plot = poly_features.fit_transform(np.linspace(-3, 3, 100).reshape(-1, 1))\n",
    "plt.plot(X, y, 'bo', markersize=3)\n",
    "plt.plot(np.linspace(-3, 3, 100), ridge_reg.predict(X_plot), 'r-')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Lasso regression\n",
    "\n",
    "#### Why Lasso results in sparsity\n",
    "\n",
    "Consider a vector $\\vec{x}=(1, \\varepsilon) \\in \\mathbb{R}^{2}$, where $\\epsilon>0$\n",
    "is small, then $l_1$ and $l_2$ norms of $\\vec{x}$ are given by\n",
    "$$\\|\\vec{x}\\|_{1}=1+\\varepsilon,\\|\\vec{x}\\|_{2}^{2}=1+\\varepsilon^{2}$$\n",
    "\n",
    "After introducing some regularization procedure, we are going to reduce the\n",
    "magnitude of one of the elements of $\\vec{x}$ by $\\delta<\\epsilon$, if we change\n",
    "$x_1$ to $1-\\delta$, then\n",
    "$$\\|\\vec{x}-(\\delta, 0)\\|_{1}=1-\\delta+\\varepsilon,\\|\\vec{x}-(\\delta, 0)\\|_{2}^{2}=1-2 \\delta+\\delta^{2}+\\varepsilon^{2}$$\n",
    "\n",
    "then reduction of norm are\n",
    "\n",
    "\\begin{align}\n",
    "\\Delta l_1 &=\\delta\\\\\n",
    "\\Delta l_2 &=2\\delta-\\delta^2\n",
    "\\end{align}\n",
    "\n",
    "On the other hand, if we reduce $x_2$, the smaller one by $\\delta$,\n",
    "$$\\|\\vec{x}-(0, \\delta)\\|_{1}=1-\\delta+\\varepsilon, \\quad\\|\\vec{x}-(0, \\delta)\\|_{2}^{2}=1-2 \\varepsilon \\delta+\\delta^{2}+\\varepsilon^{2}$$\n",
    "\n",
    "the reduction would be\n",
    "\n",
    "\\begin{align}\n",
    "    \\Delta l_1 &=\\delta\\\\\n",
    "    \\Delta l_2 &=2\\epsilon\\delta-\\delta^2\n",
    "\\end{align}\n",
    "\n",
    "Thus, when penalizing a moedel using $l_2$ norm, it is highly likely that\n",
    "anything will ever be set to zero, because $\\epsilon$ is small and $\\delta$ is even\n",
    "smaller."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "poly_features = PolynomialFeatures(degree=10)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "lasso_reg = Lasso(alpha=1)\n",
    "lasso_reg.fit(X_poly, y)\n",
    "print(lasso_reg.coef_, lasso_reg.intercept_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Logistic regression\n",
    "\n",
    "The cost function is\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\boldsymbol{\\theta})=-\\frac{1}{m} \\sum_{i=1}^{m}\\left[y^{(i)} \\log \\left(\\hat{p}^{(i)}\\right)+\\left(1-y^{(i)}\\right) \\log \\left(1-\\hat{p}^{(i)}\\right)\\right]\n",
    "\\end{equation}\n",
    "\n",
    "Derivatives is\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_{j}} \\mathrm{~J}(\\boldsymbol{\\theta})=\\frac{1}{m} \\sum_{i=1}^{m}\\left(\\sigma\\left(\\boldsymbol{\\theta}^{T} \\mathbf{x}^{(i)}\\right)-y^{(i)}\\right) x_{j}^{(i)}$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "list(iris.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "iris['feature_names']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "iris['data'][:, 3:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = iris['data'][:, 3:]\n",
    "y = (iris['target'] == 2).astype(np.int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", label=\"Iris-Virginica\")\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", label=\"Not Iris-Virginica\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(y_proba)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Softmax function\n",
    "\n",
    "$$\\hat{p}_{k}=\\sigma(\\mathbf{s}(\\mathbf{x}))_{k}=\\frac{\\exp \\left(s_{k}(\\mathbf{x})\\right)}{\\sum_{j=1}^{K} \\exp \\left(s_{j}(\\mathbf{x})\\right)}$$\n",
    "\n",
    "the prediction is\n",
    "\n",
    "$$\\hat{y}=\\underset{k}{\\operatorname{argmax}} \\sigma(\\mathbf{s}(\\mathbf{x}))_{k}=\\underset{k}{\\operatorname{argmax}} s_{k}(\\mathbf{x})=\\underset{k}{\\operatorname{argmax}}\\left(\\left(\\boldsymbol{\\theta}^{(k)}\\right)^{T} \\mathbf{x}\\right)$$\n",
    "\n",
    "the cross entropy loss is\n",
    "\n",
    "$$J(\\boldsymbol{\\Theta})=-\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y_{k}^{(i)} \\log \\left(\\hat{p}_{k}^{(i)}\\right)$$\n",
    "\n",
    "Gradient vector for class $k$\n",
    "\n",
    "$$\\nabla_{\\boldsymbol{\\theta}^{(k)}} J(\\boldsymbol{\\Theta})=\\frac{1}{m} \\sum_{i=1}^{m}\\left(\\hat{p}_{k}^{(i)}-y_{k}^{(i)}\\right) \\mathbf{x}^{(i)}$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
    "y = iris[\"target\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression(C=10, multi_class='multinomial')"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10)\n",
    "softmax_reg.fit(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}