{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Netflix Prize competition (ensemble learning)\n",
    "2. Three methods\n",
    "    1. bagging\n",
    "    2. boosting\n",
    "    3. stacking\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# toss a coin with 0.51 probability heads\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Why ensemble can do better\n",
    "Suppose you build an ensemble containing 1,000 classifiers that are individ‐ ually correct only 51% of the time (barely better than random guessing). If you pre‐ dict the majority voted class, you can hope for up to 75% accuracy! However, this is only true if all classifiers are perfectly independent, making uncorrelated errors, which is clearly not the case since they\n",
    "are trained on the same data. They are likely to make the same\n",
    "types of errors, so there will be many majority votes for\n",
    "the wrong class, reducing the ensemble’s accuracy.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X, y = mnist.data, mnist.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                    X, y, test_size=0.2, random_state=42)\n",
    "print([a.shape for a in [X_train, X_test, y_train, y_test]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.astype(np.float))\n",
    "X_test = scaler.fit_transform(X_test.astype(np.float))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "            estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "            voting='hard')\n",
    "\n",
    "voting_clf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If all classifiers are able to estimate class probabilities (i.e., they have a\n",
    "pre dict_proba() method), then you can tell Scikit-Learn to predict the class\n",
    "with the highest class probability, averaged over all the individual classifiers.\n",
    "This is called soft voting. It often achieves higher performance than hard voting because it gives more weight to highly confident votes. All you need to do is replace voting=\"hard\" with voting=\"soft\" and ensure that all classifiers can estimate class probabilities.\n",
    "1. If ‘hard’, uses predicted class labels for majority rule voting.\n",
    "2. if ‘soft’, predicts the class label based on the argmax of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers.\n",
    "\n",
    "\n",
    "### Bagging and pasting\n",
    "One way to get a diverse set of classifiers is to use very different training algorithms, as just discussed.\n",
    "Another approach is to use the same training algorithm for every predictor, but to train them on different random subsets of the training set.\n",
    "1. When sampling is performed with replacement, this method is called bagging1 (short for bootstrap aggregating2).\n",
    "2. When sampling is performed without replacement, it is called pasting.3\n",
    "\n",
    "Each individual predictor has a higher bias than if it were trained on the original training set, but aggregation reduces both bias and variance.4 Generally, the net result is that the ensemble has a similar bias but a lower variance than a single predictor trained on the original training set.\n",
    "\n",
    "#### How to plot decision boundary?\n",
    "#### what is out of bag probability: 63 vs 37?\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=100,\n",
    "    max_samples=1000, bootstrap=True, n_jobs=-1)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_true=y_test, y_pred=y_pred)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With bagging, some instances may be sampled several times for any\n",
    "given predictor, while others may not be sampled at all.\n",
    "By default a BaggingClassifier samples m training instances with\n",
    "replacement (bootstrap=True), where m is the size of the training\n",
    "set. This means that only about 63% of the training instances are\n",
    "sampled on average for each predictor. The remaining 37% of\n",
    "the training instances that are not sampled are called out-of-bag\n",
    "(oob) instances. Note that they are not the same 37% for all predictors.\n",
    "\n",
    "Since a predictor never sees the oob instances during training, it\n",
    "can be evaluated on these instances, without the need for a\n",
    "separate validation set. You can evaluate the ensemble itself\n",
    "by averaging out the oob evaluations of each predictor."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=100,\n",
    "    bootstrap=True, n_jobs=4, oob_score=True)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Random Patches and Random Subspaces\n",
    "Sampling subfeatures instead of subset of training data.\n",
    "\n",
    "1. Sampling both training instances and features is called the Random Patches method.\n",
    "2. Keeping all training instances (i.e., bootstrap=False and max_sam ples=1.0) but sampling features (i.e., bootstrap_features=True and/or max_fea tures smaller than 1.0) is called the Random Subspaces method.\n",
    "\n",
    "\n",
    "### Random forest\n",
    "\n",
    "With a few exceptions, a RandomForestClassifier has all the hyperparameters of a DecisionTreeClassifier (to control how trees are grown), plus all the hyperpara‐ meters of a BaggingClassifier to control the ensemble itself.11\n",
    "\n",
    "The Random Forest algorithm introduces extra randomness when growing trees; instead of searching for the very best feature when splitting a node (see Chapter 6), it searches for the best feature among a random subset of features. This results in a greater tree diversity, which (once again) trades a higher bias for a lower variance, generally yielding an overall better model. The following BaggingClassifier is roughly equivalent to the previous RandomForestClassifier:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators=500,\n",
    "                                 max_leaf_nodes=16,\n",
    "                                 n_jobs=-1,\n",
    "                                 max_samples=0.3,\n",
    "                                 max_features=0.5)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rnd_clf.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6505714285714286\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_true=y_test, y_pred=y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7fe284e5a790>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "importance = rnd_clf.feature_importances_\n",
    "importance = importance.reshape((28, 28))\n",
    "plt.imshow(importance)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#这个和random forest效果差不多\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "     DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n",
    "     n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Importance\n",
    "\n",
    "Yet another great quality of Random Forests is that they make it easy to measure the relative importance of each feature. Scikit-Learn measures a feature’s importance by looking at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest). More precisely, it is a weighted average, where each node’s weight is equal to the number of training samples that are associated with it"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.10483241230767104\n",
      "sepal width (cm) 0.02338246283660498\n",
      "petal length (cm) 0.4217346734134684\n",
      "petal width (cm) 0.45005045144225575\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extra-Trees\n",
    "\n",
    "Later"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Boosting\n",
    "\n",
    "Boosting (originally called hypothesis boosting) refers to any Ensemble method\n",
    "that can combine several weak learners into a strong learner. The general idea\n",
    "of most boosting methods is to train predictors sequentially, each trying to\n",
    "correct its predecessor.\n",
    "\n",
    "### AdaBoost (Adaptive Boosting)\n",
    "\n",
    "One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor underfitted. This results in new predic‐ tors focusing more and more on the hard cases.\n",
    "This is the technique used by Ada‐Boost. Once all predictors are trained, the ensemble makes predictions very much like bag‐ ging or pasting, except that predictors have different weights depending on their overall accuracy on the weighted training set.\n",
    "\n",
    "At the beginning, each instance (not predictor) weight $w_i$ is initially set to $\\frac{1}{m}$.\n",
    "A first predictor is trained and its weighted error rate $r_1$ is computed on\n",
    "the training set, see Equation\n",
    "\n",
    "$$r_j = \\frac{\\sum_{\\hat{y}_{j}^{(i)} \\neq y^{(i)}} w^{(i)}}{\\sum_i w^{(i)}}$$\n",
    "\n",
    "where $\\hat{y}_{j}^{(i)}$ is the j-th predictor’s prediction for the i-th instance.\n",
    "\n",
    "例如，如果第1个分类器有5个错，那$r_j=\\frac{5}{m}$.\n",
    "\n",
    "The predictor’s weight $\\alpha_j$ is then computed using Equation 7-2, where $\\eta$ is the\n",
    "learning rate hyperparameter (defaults to 1). The more accurate the predictor\n",
    "is, the higher its weight will be. If it is just guessing randomly, then its\n",
    "weight will be close to zero. However, if it is most often wrong (i.e., less\n",
    "accurate than random guessing), then its weight will be negative.\n",
    "\n",
    "$$\\alpha_{j}=\\eta \\log \\frac{1-r_{j}}{r_{j}}$$\n",
    "\n",
    "Next the instance weights are updated using Equation 7-3: the misclassified instances are boosted.\n",
    "\n",
    "\\begin{equation}\n",
    "w^{(i)} \\leftarrow\\left\\{\\begin{array}{ll}\n",
    "w^{(i)} & \\text { if } \\widehat{y}_{j}^{(i)}=y^{(i)} \\\\\n",
    "w^{(i)} \\exp \\left(\\alpha_{j}\\right) & \\text { if } \\widehat{y}_{j}^{(i)} \\neq y^{(i)}\n",
    "\\end{array}\\right.\n",
    "\\end{equation}\n",
    "\n",
    "如果这个分类分错了，在权重越大的分类器上分错了，增加的权重越多。那么Then all the instance weights are normalized (i.e., divided by $\\sum_{i=1}^{m} w^{(i)}$).\n",
    "\n",
    "Finally, a new predictor is trained using the updated weights, and the whole process is repeated (the new predictor’s weight is computed, the instance weights are updated, then another predictor is trained, and so on). The algorithm stops when the desired number of predictors is reached, or when a perfect predictor is found.\n",
    "\n",
    "To make predictions, AdaBoost simply computes the predictions of all the predictors and weighs them using the predictor weights $\\alpha_j$. The predicted class is the one that receives the majority of weighted votes\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y}(\\mathbf{x})=\n",
    "\\underset{k}{\\operatorname{argmax}} \\sum_{j=1 \\atop \\hat{y}_j(\\mathbf{x})=k}^{N}  p_{jk}\\alpha_{j}\n",
    "\\end{equation},   where $N$ is the number of predictors."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_clf = AdaBoostClassifier(\n",
    " DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    " algorithm=\"SAMME.R\", learning_rate=0.5)\n",
    "ada_clf.fit(X_train, y_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gradient Boosting.\n",
    "\n",
    "Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictor."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n0    -122.23     37.88                41.0        880.0           129.0   \n1    -122.22     37.86                21.0       7099.0          1106.0   \n2    -122.24     37.85                52.0       1467.0           190.0   \n3    -122.25     37.85                52.0       1274.0           235.0   \n4    -122.25     37.85                52.0       1627.0           280.0   \n\n   population  households  median_income  median_house_value ocean_proximity  \n0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n4       565.0       259.0         3.8462            342200.0        NEAR BAY  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n      <th>median_house_value</th>\n      <th>ocean_proximity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-122.23</td>\n      <td>37.88</td>\n      <td>41.0</td>\n      <td>880.0</td>\n      <td>129.0</td>\n      <td>322.0</td>\n      <td>126.0</td>\n      <td>8.3252</td>\n      <td>452600.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-122.22</td>\n      <td>37.86</td>\n      <td>21.0</td>\n      <td>7099.0</td>\n      <td>1106.0</td>\n      <td>2401.0</td>\n      <td>1138.0</td>\n      <td>8.3014</td>\n      <td>358500.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-122.24</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1467.0</td>\n      <td>190.0</td>\n      <td>496.0</td>\n      <td>177.0</td>\n      <td>7.2574</td>\n      <td>352100.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1274.0</td>\n      <td>235.0</td>\n      <td>558.0</td>\n      <td>219.0</td>\n      <td>5.6431</td>\n      <td>341300.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1627.0</td>\n      <td>280.0</td>\n      <td>565.0</td>\n      <td>259.0</td>\n      <td>3.8462</td>\n      <td>342200.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing = pd.read_csv(\"datasets/housing/housing.csv\")\n",
    "print(housing.shape)\n",
    "housing.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "y = housing['median_house_value'].to_numpy()\n",
    "X = housing.drop(['ocean_proximity', 'median_house_value'], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8) (20640,)\n"
     ]
    }
   ],
   "source": [
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X = imputer.fit_transform(X)\n",
    "X = scaler.fit_transform(X)\n",
    "print(X.shape, y.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "DecisionTreeRegressor(max_depth=2)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "DecisionTreeRegressor(max_depth=2)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the second regressor on the residual error made by the first one\n",
    "\n",
    "y2 = y_train - tree_reg1.predict(X_train)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X_train, y2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "DecisionTreeRegressor(max_depth=2)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then we train a third regressor on the residual errors made\n",
    "# by the second predictor:\n",
    "\n",
    "y3 = y2 - tree_reg2.predict(X_train)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X_train, y3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[130860.53774343 130860.53774343 318596.14241791 ... 421288.09578806\n",
      " 130860.53774343 239261.54987149]\n"
     ]
    }
   ],
   "source": [
    "y_pred = sum(tree.predict(X_test) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
    "print(y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "77807.19671402498"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.sqrt(mean_squared_error(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "59593.7802452775"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=30, learning_rate=0.5)\n",
    "gbrt.fit(X_train, y_train)\n",
    "np.sqrt(mean_squared_error(y_test, gbrt.predict(X_test)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The learning_rate hyperparameter scales the contribution of each tree. If you set it to a low value, such as 0.1, you\n",
    "will need more trees in the ensemble to fit the train‐ ing set, but the predictions will usually\n",
    "generalize better. This is a regularization tech‐ nique called shrinkage.\n",
    "Figure 7-10 shows two GBRT ensembles trained with a low learning rate:\n",
    "the one on the left does not have enough trees to fit the training set,\n",
    "while the one on the right has too many trees and overfits the training set.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15480, 8) (5160, 8) (15480,) (5160,)\n"
     ]
    }
   ],
   "source": [
    "# Early stopping\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "GradientBoostingRegressor(max_depth=8, n_estimators=200)"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=8, n_estimators=200)\n",
    "gbrt.fit(X_train, y_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n"
     ]
    },
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x7fe26ea21a60>]"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "errors = [np.sqrt(mean_squared_error(y_val, y_pred)) for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors)#%%\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)\n",
    "print(bst_n_estimators)\n",
    "plt.plot(range(len(errors)), errors, 'b-')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "gbrt.n_estimators = n_estimators\n",
    "gbrt.fit(X_train, y_train)\n",
    "y_pred = gbrt.predict(X_val)\n",
    "val_error = mean_squared_error(y_val, y_pred) if val_error < min_val_error:\n",
    "            min_val_error = val_error\n",
    "error_going_up = 0 else:\n",
    "error_going_up += 1\n",
    "if error_going_up == 5:\n",
    "break # early stopping"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The GradientBoostingRegressor class also supports a subsample hyperparameter,\n",
    "which specifies the fraction of training instances to be used for training\n",
    "each tree. For example, if subsample=0.25, then each tree is trained on 25%\n",
    "of the training instan‐ ces, selected randomly. As you can probably guess\n",
    "by now, this trades a higher bias for a lower variance. It also speeds\n",
    "up training considerably. This technique is called Stochastic Gradient\n",
    "Boosting.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is worth noting that an optimized implementation of Gradient Boosting\n",
    "is available in the popular python library XGBoost, which stands for Extreme\n",
    "Gradient Boosting. This package was initially developed by Tianqi Chen as\n",
    "part of the Distributed (Deep) Machine Learning Community (DMLC), and it\n",
    "aims at being extremely fast, scalable and portable. In fact, XGBoost\n",
    "is often an important component of the winning entries in ML competitions.\n",
    "XGBoost’s API is quite similar to Scikit-Learn’s:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: conda install [-h] [--revision REVISION] [-n ENVIRONMENT | -p PATH]\r\n",
      "                     [-c CHANNEL] [--use-local] [--override-channels]\r\n",
      "                     [--repodata-fn REPODATA_FNS] [--strict-channel-priority]\r\n",
      "                     [--no-channel-priority] [--no-deps | --only-deps]\r\n",
      "                     [--no-pin] [--copy] [-C] [-k] [--offline] [-d] [--json]\r\n",
      "                     [-q] [-v] [-y] [--download-only] [--show-channel-urls]\r\n",
      "                     [--file FILE] [--force-reinstall]\r\n",
      "                     [--freeze-installed | --update-deps | -S | --update-all | --update-specs]\r\n",
      "                     [-m] [--clobber] [--dev]\r\n",
      "                     [package_spec [package_spec ...]]\r\n",
      "conda install: error: argument -c/--channel: expected one argument\r\n"
     ]
    }
   ],
   "source": [
    "! conda install -c -y conda-forge xgboost"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "import xgboost\n",
    "xgb_reg = xgboost.XGBRegressor()\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred = xgb_reg.predict(X_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:173049.51562\n",
      "[1]\tvalidation_0-rmse:130653.92188\n",
      "[2]\tvalidation_0-rmse:102304.36719\n",
      "[3]\tvalidation_0-rmse:84829.88281\n",
      "[4]\tvalidation_0-rmse:73296.75000\n",
      "[5]\tvalidation_0-rmse:66490.70312\n",
      "[6]\tvalidation_0-rmse:62210.29688\n",
      "[7]\tvalidation_0-rmse:59900.10547\n",
      "[8]\tvalidation_0-rmse:57965.00391\n",
      "[9]\tvalidation_0-rmse:56495.08594\n",
      "[10]\tvalidation_0-rmse:55466.91406\n",
      "[11]\tvalidation_0-rmse:53366.54297\n",
      "[12]\tvalidation_0-rmse:52798.69141\n",
      "[13]\tvalidation_0-rmse:52339.94922\n",
      "[14]\tvalidation_0-rmse:51990.00391\n",
      "[15]\tvalidation_0-rmse:51933.79688\n",
      "[16]\tvalidation_0-rmse:51338.07812\n",
      "[17]\tvalidation_0-rmse:51068.67188\n",
      "[18]\tvalidation_0-rmse:51001.62891\n",
      "[19]\tvalidation_0-rmse:50734.23828\n",
      "[20]\tvalidation_0-rmse:50434.51172\n",
      "[21]\tvalidation_0-rmse:50376.27344\n",
      "[22]\tvalidation_0-rmse:50210.96484\n",
      "[23]\tvalidation_0-rmse:50111.66406\n",
      "[24]\tvalidation_0-rmse:49815.51172\n",
      "[25]\tvalidation_0-rmse:49629.18750\n",
      "[26]\tvalidation_0-rmse:49634.97266\n",
      "[27]\tvalidation_0-rmse:49637.30859\n"
     ]
    }
   ],
   "source": [
    "# XGBoost also offers several nice features, such as automatically taking care of early stopping:\n",
    "xgb_reg.fit(X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)], early_stopping_rounds=2)\n",
    "y_pred = xgb_reg.predict(X_val)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stacking\n",
    "The last Ensemble method we will discuss in this chapter is called stacking\n",
    "(short for stacked generalization).18 It is based on a simple idea: instead\n",
    "of using trivial functions (such as hard voting) to aggregate the predictions\n",
    "of all predictors in an ensemble, why don’t we train a model to perform this\n",
    "aggregation? Figure 7-12 shows such an ensemble performing a regression\n",
    "task on a new instance. Each of the bottom three predictors predicts a\n",
    "different value (3.1, 2.7, and 2.9), and then the final predictor (called\n",
    "a blender, or a meta learner) takes these predictions as inputs and\n",
    "makes the final prediction (3.0)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}